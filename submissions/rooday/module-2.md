### Nature of the incident

A few years ago I used to run a Garry's Mod server, and as I'm sure you may know, video games are one of the greatest targets for trolling. The gamemode the server used was basically a version of Mafia where innocent players had to figure out who the traitors were before they killed them, and as such was very prone to trolls just coming in and ending games early by killing everyone. On top of that, games were prone to people typing offensive messages in the chat, or saying offensive things to others over voice chat, and all manner of other forms of griefing.

### Moderation action

There's no particular incident that sticks out in particular as they're all pretty much the same, so I'll go over the procedure I had come to use to deal with issues on the server. That procedure heavily relied on a moderation tool called ULX, which provides utilities to handle any kind of troll. For instance, if a user started typing or saying offensive things into the chat, they could be "muted" or "gagged" to prevent them from typing and talking, respectively. However, they would still have access to a chatline that only admins could see, and so could ask to be unmuted or ungagged after they've calmed down. If a player killed another player unfairly (the gamemode had a very strict ruleset) and it was obvious they did so intentionally, they would be "slayed" the next round, essentially putting them in a time out where they can only watch the round progress instead of playing. If a player continued to misbehave and cause problems, they'd be kicked from the server, which usually was the end for most trolls (In addition to an admin being able to kick players, users had access to a vote-kick system, so they could still maintain a sense of order even when admins were offline). Every now and then, there would be a particularly persistent troll that would return after being kicked and continue to wreak havoc, in which case a ban would have to issued. Bans ranged from a few minutes to permanently, and would be adjusted depending on the severity of the offense (however the default 3 day ban was fairly effective). 

However, the actions described above can only be taken once it's certain that someone committed an offense. In Garry's Mod it's not always so easy to be sure, especially in cases of a player being killed out of turn. In order to resolve those cases, an admin would would have to listen to both sides of the story and make their best judgement. But that method isn't very accurate, and so I installed another moderation tool called Advanced Damagelogs. It kept track of who damaged who, with what, and when, and what their roles were (if they were innocent or traitor, etc). It also came with a report feature, so if a user believed they were killed out of turn, they could file a report that would first prompt the offender for their side of the story, and then show both sides to an admin. This way, admins could easily gather the information they needed to make a moderation decision.

### Outcome

The actions outlined above for dealing with trolls proved to be very efficient in keeping the community in order. Without random trolls ruining the game, the community was able to steadily grow and maintain a roster of regular players that would play every day. The server became more than just a place to play a game, and was a real community where people knew each other. 

### What could be different?

Although the community did pretty well with the moderation, there's always room for improvement. I don't run that server anymore, but in retrospect I'd say that sometimes bans and kicks were uncalled for. There were definitely instances were a slay would've sufficed, as forcing a player to sit out a round would be enough of a deterrent. But admins are just normal people who are prone to having bad days, and as such some unfair moderation actions were probably made. In order to prevent that, having multiple admins decide on an action would help, but that is limited by being able to have enough trustworthy admins online at the same time.

### Application to Other Online Communities

Outside of games, online communities usually take the form of some sort of channel for communication. Examples that my student group uses include Slack, Facebook, and email. While moderation concepts are the same regardless of the platform, it's important to note that simply due to the nature of games and the increased amount of rules they have, they tend to have better/more specific moderation tools. Platforms like Facebook and Slack don't really offer much in the way of moderation (partly because they don't need mcuh) other than deleting posts and removing members from groups/channels (emails offer nothing, other than having every member individually block an offender, which isn't that practical). However, Slack has a fairly robust API, so it woulnd't be out of the realm of possibility to develop moderation tools. Even better, one could probably automate many moderation functions, such as removing/hiding messages that have offensive content. Perhaps such work could be done by GitHub's friendly robot sidekick...
